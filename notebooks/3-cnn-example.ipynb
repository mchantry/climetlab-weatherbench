{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN\n",
    "\n",
    "In this notebook we will go through all the steps required to train a fully convolutional neural network. Because this takes a while and uses a lot of GPU RAM a separate command line script (`train_nn.py`) is also provided in the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import climetlab as cml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def limit_mem():\n",
    "    \"\"\"By default TF uses all available GPU memory. This function prevents this.\"\"\"\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:25:42.011368: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-06 13:25:42.011813: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geopotential', 500)]\n",
      "[('temperature', 850)]\n",
      "[('geopotential', 500)]\n",
      "[('temperature', 850)]\n",
      "[('geopotential', 500)]\n",
      "[('temperature', 850)]\n"
     ]
    }
   ],
   "source": [
    "features_names = ['geopotential_500','temperature_850']\n",
    "train = {f:cml.load_dataset('weatherbench-extended', f, year = ['2015']) for f in features_names}\n",
    "#train = {f:cml.load_dataset('weatherbench-extended', f, year = ['2015', '2016']) for f in features_names}\n",
    "valid = {f:cml.load_dataset('weatherbench-extended', f, year = ['2017']) for f in features_names}\n",
    "test =  {f:cml.load_dataset('weatherbench-extended', f, year = ['2018']) for f in features_names}\n",
    "\n",
    "# Could also use: climetlab>=0.13.2\n",
    "# ds = cml.load_dataset('weatherbench-extended', features_names, year = ['2017'])\n",
    "# len(ds), len(ds.sel(param='t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'geopotential_500': <climetlab_weatherbench.extended.WeatherbenchExtendedCDS object at 0x2d69471c0>, 'temperature_850': <climetlab_weatherbench.extended.WeatherbenchExtendedCDS object at 0x2a4dcb550>}\n",
      "[1460, 1460]\n",
      "1454\n",
      "1454\n",
      "1454\n"
     ]
    }
   ],
   "source": [
    "bs = 32\n",
    "lead_time = 6\n",
    "\n",
    "offset = 6\n",
    "\n",
    "\n",
    "def add_offset(ds, offset):\n",
    "    def ds_with_offset(i):\n",
    "        return ds[i + offset].to_numpy()\n",
    "\n",
    "    return ds_with_offset\n",
    "\n",
    "\n",
    "def features_to_targets(data):\n",
    "    return [add_offset(f, offset) for f in data]\n",
    "\n",
    "\n",
    "print(train)\n",
    "print([len(v) for k, v in train.items()])\n",
    "\n",
    "\n",
    "def to_tfdataset(features):\n",
    "\toptions = [dict(normalize=\"min-max\") for f, feature in features.items()]\n",
    "\t# options = [dict(normalize='min-max', feature.mean, feature.std) for f, feature in train.items()]\n",
    "\n",
    "\t# targets = {f: add_offset(feature, offset) for f, feature in features.items()}\n",
    "\ttargets = features\n",
    "\ttarget_options = [dict(normalize=\"min-max\") for f, feature in features.items()]\n",
    "\t# target_options = [dict(normalize=\"min-max\", offset=offset) for f, feature in features.items()]\n",
    "\n",
    "\ttotal_length = len(list(features.values())[0]) - offset\n",
    "\tprint(total_length)\n",
    "\n",
    "\tfirst_feature = list(features.values())[0]\n",
    "\treturn first_feature.to_tfdataset(\n",
    "\t    features=list(features.values()),\n",
    "\t    targets=list(targets.values()),\n",
    "\t    options=options,\n",
    "\t    target_options=target_options,\n",
    "\t    total_length=total_length,\n",
    "\t)\n",
    "\n",
    "tfds_train = to_tfdataset(train)\n",
    "tfds_valid = to_tfdataset(valid)\n",
    "tfds_test = to_tfdataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 33, 64), (2, 33, 64)]\n"
     ]
    }
   ],
   "source": [
    "for i in tfds_train.as_numpy_iterator():\n",
    "\tprint([_.shape for _ in i])\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds_train._climetlab_tf_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create and train model\n",
    "\n",
    "Next up, we need to create the model architecture. Here we will use a fully connected convolutional network. Because the Earth is periodic in longitude, we want to use a periodic convolution in the lon-direction. This is not implemented in Keras, so we have to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "\n",
    "# from climetlab.ml.tf import PeriodicConv2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "\n",
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [\n",
    "                inputs[:, :, -self.pad_width :, :],\n",
    "                inputs,\n",
    "                inputs[:, :, : self.pad_width, :],\n",
    "            ],\n",
    "            axis=2,\n",
    "        )\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(\n",
    "            inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]]\n",
    "        )\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"pad_width\": self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        conv_kwargs={},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert (\n",
    "                kernel_size[0] == kernel_size[1]\n",
    "            ), \"PeriodicConv2D only works for square kernels\"\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(filters, kernel_size, padding=\"valid\", **conv_kwargs)\n",
    "\n",
    "\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def build_cnn(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    import tensorflow.keras as keras\n",
    "    from tensorflow.keras.layers import Input, Dropout,LeakyReLU\n",
    "    #from climetlab.ml.tf import PeriodicConv2D\n",
    "\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        if dr > 0:\n",
    "            x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn = build_cnn([64, 2], [5, 5], (32, 64, 2))\n",
    "# cnn = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn.compile(tf.keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 64, 2)]       0         \n",
      "                                                                 \n",
      " periodic_conv2d_2 (Periodic  (None, 32, 64, 2)        0         \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32, 64, 2)         0         \n",
      "                                                                 \n",
      " periodic_conv2d_3 (Periodic  (None, 32, 64, 2)        0         \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:28:33.193564: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449/1460 [============================>.] - ETA: 0s - loss: 1464027776.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:28:41.368338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460/1460 [==============================] - 15s 10ms/step - loss: 1463950336.0000 - val_loss: 1467552768.0000\n",
      "Epoch 2/100\n",
      "1460/1460 [==============================] - 14s 10ms/step - loss: 1463950720.0000 - val_loss: 1467552768.0000\n",
      "Epoch 3/100\n",
      "1460/1460 [==============================] - 14s 10ms/step - loss: 1463950208.0000 - val_loss: 1467552768.0000\n",
      "Epoch 3: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2abb92200>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we didn't load the full data this is only for demonstration.\n",
    "cnn.fit(tfds_train, epochs=100, validation_data=tfds_valid, \n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=2,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn.save_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load weights from externally trained model\n",
    "cnn.load_weights('cnn.h5')\n",
    "# cnn.load_weights('cnn_good.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create predictions\n",
    "\n",
    "Now that we have our model we need to create a prediction NetCDF file. This function does this. \n",
    "\n",
    "We can either directly predict the target lead time (e.g. 5 days) or create an iterative forecast by chaining together many e.g. 6h forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create predictions for non-iterative model\"\"\"\n",
    "    preds = model.predict_generator(dg)\n",
    "    # Unnormalize\n",
    "    preds = preds * dg.std.values + dg.mean.values\n",
    "    fcs = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.var_dict.items():\n",
    "        if levels is None:\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mx/qjhp3qsj77zfb1x4vj6d6xtr0000gn/T/ipykernel_26768/2368901885.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  preds = model.predict_generator(dg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PrefetchDataset' object has no attribute 'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fc \u001b[39m=\u001b[39m create_predictions(cnn, tfds_test\u001b[39m.\u001b[39;49m_climetlab_tf_input)\n",
      "Cell \u001b[0;32mIn [40], line 5\u001b[0m, in \u001b[0;36mcreate_predictions\u001b[0;34m(model, dg)\u001b[0m\n\u001b[1;32m      3\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_generator(dg)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Unnormalize\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m preds \u001b[39m=\u001b[39m preds \u001b[39m*\u001b[39m dg\u001b[39m.\u001b[39;49mstd\u001b[39m.\u001b[39mvalues \u001b[39m+\u001b[39m dg\u001b[39m.\u001b[39mmean\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m fcs \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m lev_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute 'std'"
     ]
    }
   ],
   "source": [
    "fc = create_predictions(cnn, tfds_test._climetlab_tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_weighted_rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compute_weighted_rmse(fc, valid)\u001b[39m.\u001b[39mcompute()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_weighted_rmse' is not defined"
     ]
    }
   ],
   "source": [
    "compute_weighted_rmse(fc, valid).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_iterative_predictions(model, dg, max_lead_time=5*24):\n",
    "    state = dg.data[:dg.n_samples]\n",
    "    preds = []\n",
    "    for _ in range(max_lead_time // dg.lead_time):\n",
    "        state = model.predict(state)\n",
    "        p = state * dg.std.values + dg.mean.values\n",
    "        preds.append(p)\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    lead_time = np.arange(dg.lead_time, max_lead_time + dg.lead_time, dg.lead_time)\n",
    "    das = []; lev_idx = 0\n",
    "    for var, levels in dg.var_dict.items():\n",
    "        if levels is None:\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, :, lev_idx],\n",
    "                dims=['lead_time', 'time', 'lat', 'lon'],\n",
    "                coords={'lead_time': lead_time, 'time': dg.init_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, :, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['lead_time', 'time', 'lat', 'lon', 'level'],\n",
    "                coords={'lead_time': lead_time, 'time': dg.init_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "fc_iter = create_iterative_predictions(cnn, tfds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "rmse = evaluate_iterative_forecast(fc_iter, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "rmse.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "rmse.z_rmse.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "rmse.t_rmse.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4270e1ac5bef27d5e6a33f4883da48ffc137d9a63a21d2e4dad8cf8937cea50e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
